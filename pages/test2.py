# python -m streamlit run Home.py
import streamlit as st
from core.utils import *
from streamlit_image_select import image_select
from streamlit_tags import st_tags

setting()

# llm ê´€ë ¨
from pathlib import Path 
from langchain_openai import ChatOpenAI 
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from operator import itemgetter
#-------------------------------------------------------------------
# Settings
#-------------------------------------------------------------------
key_expander = "isexpanded"
key_chain = "demo_chain"
key_memory = "demo_memory"
key_history = "demo_history"

if "chat_start" not in st.session_state:
    st.session_state["chat_start"] = False

def chat_start():
    st.session_state["chat_start"] = True
    try:
        del st.session_state[key_history]
        del st.session_state[key_memory]
        del st.session_state[key_chain]
    except:
        pass
#-------------------------------------------------------------------
# Header
#-------------------------------------------------------------------
with st.expander(
        label=":gear: Settigns"
    ):  

    # Print Template
    persona = st.text_area(
        label="TEMPLATE",
            value="",
            height=300
    )
    
    # Start Button
    start_btn = st.button(
        label="CHAT START",
        use_container_width=True,
        type="primary",
        on_click=chat_start
    )

if st.session_state["chat_start"]:
    #-------------------------------------------------------------------
    # Make Chain
    #-------------------------------------------------------------------
    # ì±„íŒ…ì„ ì´ì–´ë‚˜ê°ˆ ë•Œ
    if key_chain in st.session_state:
        chain = st.session_state[key_chain]
        memory = st.session_state[key_memory]
        history = st.session_state[key_history]
    # ìƒˆë¡œìš´ í…œí”Œë¦¿ì„ ì ìš©í•  ë•Œ
    else:
        # ì´ˆê¸°í™” 
        st.session_state[key_history] = []

        # ë©”ëª¨ë¦¬ ì„¤ì •
        memory = ConversationBufferMemory(
            return_messages=True, 
            memory_key="chat_history"
        )
        # í”„ë¡¬í”„íŠ¸ ì„¤ì •
        # template = read_prompt("./static/templates/Demo.prompt")
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", persona),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
            ]
        )
        # ì²´ì¸ ë§Œë“¤ê¸°
        runnable = RunnablePassthrough.assign(
            chat_history = RunnableLambda(memory.load_memory_variables)
            | itemgetter("chat_history")
        )
        model = ChatOpenAI(model_name="gpt-3.5-turbo")
        output_parser = StrOutputParser()
        runnable = RunnablePassthrough.assign(
                chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("chat_history")
            )
        chain = runnable | prompt | model | StrOutputParser()
        # ì„¸ì…˜ ì •ë³´ ì €ì¥
        st.session_state[key_chain] = chain
        st.session_state[key_memory] = memory
    #-------------------------------------------------------------------
    # Chat Messages
    #-------------------------------------------------------------------
    # ì²« ì±„íŒ…ì„ ì‹œì‘í•  ë•Œ ì²« ì¸ì‚¬ ì¶œë ¥
    if len(st.session_state[key_history]) == 0:
        greeting = "ì•ˆë…•í•˜ì„¸ìš”ğŸ˜‹"
        st.chat_message("assistant").markdown(greeting)
        st.session_state[key_history].append(
            {"role":"assistant", "content": greeting}
        )
    # ì±„íŒ… ê¸°ë¡ì´ ìˆì„ ë•Œ ê¸°ë¡ëœ ì±„íŒ… ì¶œë ¥
    else:
        for chat in st.session_state[key_history]:
            st.chat_message(chat["role"]).markdown(chat["content"])

    # ì±„íŒ…ì°½ ì…ë ¥
    question = st.chat_input(placeholder="ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

    if question:
        # ì…ë ¥ëœ ì±„íŒ… ì¶œë ¥
        st.chat_message("user").markdown(question)
        st.session_state[key_history].append(
            {"role":"user", "content":question}
        )
        # ë‹µë³€ ì¶œë ¥
        with st.chat_message("assistant"):
            container = st.empty()
            answer = ""
            inputs = {
                "input": question
            }
            # print(chain)
            
            for token in chain.stream(inputs):
                answer += token
                container.markdown(answer)
        
        st.session_state[key_history].append(
            {"role":"assistant", "content":answer}
        )
        # ë©”ëª¨ë¦¬ ì €ì¥
        memory.save_context(
            {"inputs": question},
            {"output": answer}
        )

        # ë©”ëª¨ë¦¬ ì¶œë ¥
        # print(memory.load_memory_variables({}))
    