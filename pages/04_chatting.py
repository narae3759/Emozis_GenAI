# # python -m streamlit run Home.py
# import streamlit as st
# from core.utils import *
# from streamlit_image_select import image_select
# from streamlit_tags import st_tags

# setting()

# # llm ê´€ë ¨
# from pathlib import Path 
# from langchain_openai import ChatOpenAI 
# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
# from langchain_core.output_parsers import StrOutputParser
# from langchain.memory import ConversationBufferMemory
# from langchain_core.runnables import RunnableLambda, RunnablePassthrough
# from operator import itemgetter

# #-------------------------------------------------------------------
# # Settings
# #-------------------------------------------------------------------
# key_expander = "isexpanded"
# key_chain = "demo_chain"
# key_memory = "demo_memory"
# key_history = "demo_history"
# #-------------------------------------------------------------------
# # Sidebar
# #-------------------------------------------------------------------
# img_url = "https://i.postimg.cc/XJ9gFk48/openart-image-CKz-Cdiz-L-1724052914963-raw.jpg"
# with st.sidebar:
#     columns = st.columns(spec=[0.3,0.4,0.3])
#     with columns[1]:
#         st.image(img_url)
#     st.markdown("hello")
# #-------------------------------------------------------------------
# # Make Chain
# #-------------------------------------------------------------------
# # ì±„íŒ…ì„ ì´ì–´ë‚˜ê°ˆ ë•Œ
# if key_chain in st.session_state:
#     chain = st.session_state[key_chain]
#     memory = st.session_state[key_memory]
#     history = st.session_state[key_history]
# # ìƒˆë¡œìš´ í…œí”Œë¦¿ì„ ì ìš©í•  ë•Œ
# else:
#     # ì´ˆê¸°í™” 
#     st.session_state[key_history] = []

#     # ë©”ëª¨ë¦¬ ì„¤ì •
#     memory = ConversationBufferMemory(
#         return_messages=True, 
#         memory_key="chat_history"
#     )
#     # í”„ë¡¬í”„íŠ¸ ì„¤ì •
#     # template = read_prompt("./static/templates/Demo.prompt")
#     prompt = ChatPromptTemplate.from_messages(
#         [
#             ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì‚¬ëŒì…ë‹ˆë‹¤"),
#             MessagesPlaceholder(variable_name="chat_history"),
#             ("human", "{input}"),
#         ]
#     )
#     # ì²´ì¸ ë§Œë“¤ê¸°
#     runnable = RunnablePassthrough.assign(
#         chat_history = RunnableLambda(memory.load_memory_variables)
#         | itemgetter("chat_history")
#     )
#     model = ChatOpenAI(model_name="gpt-3.5-turbo")
#     output_parser = StrOutputParser()
#     runnable = RunnablePassthrough.assign(
#             chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("chat_history")
#         )
#     chain = runnable | prompt | model | StrOutputParser()
#     # ì„¸ì…˜ ì •ë³´ ì €ì¥
#     st.session_state[key_chain] = chain
#     st.session_state[key_memory] = memory
# #-------------------------------------------------------------------
# # Chat Messages
# #-------------------------------------------------------------------
# # ì²« ì±„íŒ…ì„ ì‹œì‘í•  ë•Œ ì²« ì¸ì‚¬ ì¶œë ¥
# if len(st.session_state[key_history]) == 0:
#     greeting = "ì•ˆë…•í•˜ì„¸ìš”ğŸ˜‹"
#     st.chat_message("assistant").markdown(greeting)
#     st.session_state[key_history].append(
#         {"role":"assistant", "content": greeting}
#     )
# # ì±„íŒ… ê¸°ë¡ì´ ìˆì„ ë•Œ ê¸°ë¡ëœ ì±„íŒ… ì¶œë ¥
# else:
#     for chat in st.session_state[key_history]:
#         st.chat_message(chat["role"]).markdown(chat["content"])

# # ì±„íŒ…ì°½ ì…ë ¥
# question = st.chat_input(placeholder="ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

# if question:
#     # ì…ë ¥ëœ ì±„íŒ… ì¶œë ¥
#     st.chat_message("user").markdown(question)
#     st.session_state[key_history].append(
#         {"role":"user", "content":question}
#     )
#     # ë‹µë³€ ì¶œë ¥
#     with st.chat_message("assistant"):
#         container = st.empty()
#         answer = ""
#         inputs = {
#             "input": question
#         }
#         print(chain)
        
#         for token in chain.stream(inputs):
#             answer += token
#             container.markdown(answer)
    
#     st.session_state[key_history].append(
#         {"role":"assistant", "content":answer}
#     )
#     # ë©”ëª¨ë¦¬ ì €ì¥
#     memory.save_context(
#         {"inputs": question},
#         {"output": answer}
#     )

#     # ë©”ëª¨ë¦¬ ì¶œë ¥
#     # print(memory.load_memory_variables({}))
